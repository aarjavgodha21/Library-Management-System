import requests
from bs4 import BeautifulSoup
import csv
import time
import random  # Added for random delays

def amazon_assignment(retries=3):
  for attempt in range(retries):
    try:
      # Respect robots.txt and terms of service (check manually)
      # Consider using a dedicated scraping library/API for better handling

      # Add a random delay between requests (0.5-2 seconds) to mimic human behavior
      delay = random.uniform(0.5, 2)
      time.sleep(delay)
      print(f"Waiting {delay:.2f} seconds before next request...")

      # Fetch the webpage with user-agent header to avoid detection as a bot
      headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'}
      response = requests.get('https://www.amazon.com/Best-Sellers/zgbs', headers=headers)
      response.raise_for_status()  # Check if request was successful

      # Parse the HTML content
      soup = BeautifulSoup(response.text, 'html.parser')

      # List to store products
      products = []

      # Extract product names and prices (adapt selectors if needed)
      for product in soup.find_all('div', class_='p13n-sc-truncate'):
        name = product.text.strip()
        price_element = product.find_next('span', class_='p13n-sc-price')
        price = price_element.text.strip() if price_element else 'N/A'
        products.append({'name': name, 'price': price})

      return products
    except requests.exceptions.HTTPError as http_err:
      print(f"HTTP error occurred: {http_err}")
    except Exception as e:
      print(f"An error occurred while scraping Amazon: {e}")
    # Exponential backoff before retrying
    delay = 2 ** attempt
    print(f"Retrying in {delay} seconds...")
    time.sleep(delay)

  print("Failed to scrape Amazon after multiple attempts.")
  return []

# Function to save data to CSV
def save_to_csv(products):
  try:
    with open('amazon_best_sellers.csv', 'w', newline='', encoding='utf-8') as csvfile:
      fieldnames = ['name', 'price']
      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

      writer.writeheader()
      writer.writerows(products)
      print("Data saved to amazon_best_sellers.csv successfully!")
  except Exception as e:
    print("An error occurred while saving to CSV:", e)

# Main function
if __name__ == '__main__':
  products = amazon_assignment()
  if products:
    save_to_csv(products)
  else:
    print("No products scraped.")
